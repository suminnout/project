import time
from typing import List, Tuple, Dict, Any
import numpy as np
from sentence_transformers import SentenceTransformer, util
import weaviate
import torch
from tqdm import tqdm  # âœ… ì´ê±¸ë¡œ ìˆ˜ì •ë°˜
from news_search import search_realtime_news

from google import genai
from google.genai import types

# âœ… Weaviate í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
client = weaviate.Client("http://localhost:8080")

# âœ… ì„ë² ë”© ëª¨ë¸ (ê³ ì •ë„ í•œêµ­ì–´)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
embed_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
embed_model.to(device) 

# âœ… google gemini í™œìš©
genai_client = genai.Client(api_key="XXXXXXXXXXXXX")

#################################################################################################

def _build_where_ids(id_list):
    return {
        "operator": "Or",
        "operands": [
            {"path": ["id"], "operator": "Equal", "valueString": _id}
            for _id in id_list
        ]
    }

INDEX_CLASS = "k_league"
NEAR_CERTAINTY = 0.7
TOPK_STAGE1 = 20   # 1ì°¨ ê²€ìƒ‰
TOPK_FINAL = 7     # ìµœì¢… ê²€ìƒ‰ ë¬¸ì„œ ê°œìˆ˜
LATE_FUSION_ALPHA = 0.6
LOW_CONF_FALLBACK = 0.25  # top_score < 0.25ë©´ ë¦¬íŒŒì¸ í´ë°±

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (ê³µìš©) ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _extract_doc_text(doc: Dict[str, Any]) -> str:
    """content í˜¹ì€ table_json ì¤‘ ì¡´ì¬í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜."""
    return (doc.get("content") or doc.get("table_json") or "").strip()

def _by_id(docs: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    return {(d.get("_additional") or {}).get("id", ""): d for d in docs}
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) ì§ˆë¬¸ ë²ˆì—­ (êµ­ì œëŒ€íšŒ ì£¼ì œ ì„ íƒ ì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_query_for_stage1(user_query: str, *, index_class: str = INDEX_CLASS) -> str:
    """
    1ì°¨ ê²€ìƒ‰ ì „ì— ì¿¼ë¦¬ë¥¼ ì •ê·œí™”:
    - index_classê°€ 'international'ì´ë©´ ì˜ì–´/ê¸°íƒ€ ì–¸ì–´ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ ë°˜í™˜
    - ê·¸ ì™¸ ì£¼ì œë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - ì´ë¯¸ ì˜ì–´ë©´ ë²ˆì—­ì„ ê±´ë„ˆëœ€
    - ë²ˆì—­ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œì—ë„ ì›ë¬¸ìœ¼ë¡œ ì•ˆì „ í´ë°±
    """
    cls = (index_class or "").lower()  # ì£¼ì œ ë¬¸ìì—´ì„ ì†Œë¬¸ìë¡œ ì •ê·œí™”(ì˜ˆ: 'International' â†’ 'international')
    if cls != "international":         # êµ­ì œ ê·œì •ì´ ì•„ë‹Œ ê²½ìš°ì—ëŠ”
        return user_query              # ì›ë¬¸ ê·¸ëŒ€ë¡œ 1ì°¨ ê²€ìƒ‰ìœ¼ë¡œ ë„˜ê¹€

    try:
        prompt = (
            "ì•„ë˜ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ¬ìš´ **ì˜ì–´**ë¡œ ë²ˆì—­í•˜ì„¸ìš”. "
            "ì„¤ëª…ì´ë‚˜ ë”°ì˜´í‘œ ì—†ì´ ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥:\n\n" + user_query
        )
        out = genai_client.models.generate_content(
            model="gemini-1.5-flash",
            contents=prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=256,
                temperature=0.0,
            ),
        )
        # google-genai ì‘ë‹µ íŒŒì‹±: ìš°ì„  out.text, ì—†ìœ¼ë©´ candidates ê²½ë¡œ í´ë°±
        text = getattr(out, "text", None)
        if not text:
            cands = getattr(out, "candidates", None)
            if cands and getattr(cands[0], "content", None) and getattr(cands[0].content, "parts", None):
                first_part = cands[0].content.parts[0]
                text = getattr(first_part, "text", None)
        return (text or user_query).strip()
    except Exception:
        return user_query
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) 1ì°¨ ê²€ìƒ‰ (HNSW Top-20 + distance)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def stage1_retrieve(
    user_query: str,
    *,
    index_class: str = INDEX_CLASS,
    near_certainty: float = NEAR_CERTAINTY,
    topk: int = TOPK_STAGE1
) -> Tuple[np.ndarray, List[Tuple[str, float]], List[str]]:
    """
    ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± í›„ HNSW 1ì°¨ ê²€ìƒ‰ì„ ìˆ˜í–‰.
    ë°˜í™˜: (query_vec, ids_stage1[(id, distance)], id_list)
    """
    query_vec = embed_model.encode(user_query, convert_to_numpy=True)

    res = client.query.get(
        index_class,
        ["_additional { id distance }"]
    ).with_near_vector({"vector": query_vec, "certainty": near_certainty}) \
        .with_limit(topk).do()

    items = res["data"]["Get"][index_class]
    ids_stage1 = [(it["_additional"]["id"], it["_additional"].get("distance", None)) for it in items]
    id_list = [i for i, _ in ids_stage1]
    return query_vec, ids_stage1, id_list
    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ë¦¬ë­í‚¹ (ì½”ì‚¬ì¸ + ANN distance late-fusion) ë° í´ë°± íŒë‹¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_candidates_by_ids(
    id_list: List[str],
    *,
    index_class: str = INDEX_CLASS
) -> List[Dict[str, Any]]:
    """1ì°¨ í›„ë³´ idë“¤ì„ OR whereë¡œ ë³¸ë¬¸ ì¡°íšŒ."""
    if not id_list:
        return []
    
    cands = client.query.get(
        index_class,
        ["content", "table_json", "_additional { id }"]
    ).with_where(_build_where_ids(id_list)).with_limit(len(id_list)).do()["data"]["Get"][index_class]
    return cands

def rerank_with_late_fusion(
    query_vec: np.ndarray,
    ids_stage1: List[Tuple[str, float]],
    candidates: List[Dict[str, Any]],
    *,
    alpha: float = LATE_FUSION_ALPHA,
    topk: int = TOPK_FINAL
) -> Tuple[List[str], float]:
    """
    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ + (ì •ê·œí™”í•œ) ANN distanceì˜ late-fusionìœ¼ë¡œ ì¬ë­í‚¹.
    ë°˜í™˜: (top_ids, top_score)
    """

    cand_texts, cand_ids = [], []
    for d in candidates:
        txt = _extract_doc_text(d)
        cid = (d.get("_additional") or {}).get("id", "")
        if txt and cid:
            cand_texts.append(txt)
            cand_ids.append(cid)

    if not cand_texts:
        return [], 0.0

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    doc_vecs = embed_model.encode(cand_texts, convert_to_numpy=True)
    q = torch.tensor(query_vec).unsqueeze(0)
    d = torch.tensor(doc_vecs)
    cos_scores = util.cos_sim(q, d)[0].cpu().numpy()  # [-1,1] â†’ ë‚˜ì¤‘ì— [0,1]ë¡œ ì •ê·œí™”

    # HNSW distance â†’ ìœ ì‚¬ë„ë¡œ ë³€í™˜
    dist_map = {i: (dist if dist is not None else 1.0) for i, dist in ids_stage1}
    dists = np.array([dist_map.get(cid, 1.0) for cid in cand_ids])
    if (dists.max() - dists.min()) < 1e-9:
        ann_sims = np.ones_like(dists)  # ëª¨ë‘ ë™ì¼ ê°’ì´ë©´ ë™ì  ì²˜ë¦¬
    else:
        ann_sims = 1.0 - (dists - dists.min()) / (dists.max() - dists.min())

    # ì½”ì‚¬ì¸ [-1,1] â†’ [0,1]
    cos_norm = (cos_scores + 1.0) / 2.0

    # late fusion
    fused = alpha * cos_norm + (1 - alpha) * ann_sims
    order = np.argsort(-fused)
    topk = min(topk, len(order))
    top_ids = [cand_ids[i] for i in order[:topk]]
    top_score = float(fused[order[0]])
    return top_ids, top_score

def refine_near_vector_fallback(
    id_list: List[str],
    query_vec: np.ndarray,
    *,
    index_class: str = INDEX_CLASS,
    near_certainty: float = NEAR_CERTAINTY,
    topk: int = TOPK_FINAL
) -> List[Dict[str, Any]]:
    """ì €ì‹ ë¢° ì‹œ, ë™ì¼ id í›„ë³´ ë²”ìœ„ì—ì„œ near_vectorë¡œ 2ì°¨ ë¦¬íŒŒì¸."""
    final = client.query.get(
        index_class,
        ["title","chapter_title","section_heading","content","table_json","_additional { id distance }"]
    ).with_where(_build_where_ids(id_list)) \
    .with_near_vector({"vector": query_vec, "certainty": near_certainty}) \
    .with_limit(topk).do()
    return final["data"]["Get"][index_class]

def fetch_final_docs_in_order(
    top_ids: List[str],
    *,
    index_class: str = INDEX_CLASS
) -> List[Dict[str, Any]]:
    """ìµœì¢… ìƒìœ„ idë¡œ ë¬¸ì„œë¥¼ ì¬ì¡°íšŒí•˜ê³ , top_ids ìˆœì„œë¥¼ ë³µì›."""
    if not top_ids:
        return []

    final_res = client.query.get(
        index_class,
        ["title","chapter_title","section_heading","content","table_json","_additional { id distance }"]
    ).with_where(_build_where_ids(top_ids)).with_limit(len(top_ids)).do()
    temp_docs = final_res["data"]["Get"][index_class]
    bid = _by_id(temp_docs)
    return [bid[i] for i in top_ids if i in bid]
       
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2-1) ë‰´ìŠ¤-ë¬¸ì„œ ìœ ì‚¬ë„ ë¹„êµ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_filtered_news_for_docs(user_query: str, docs: list, *, max_keep: int = 5):
    # 1) ë‰´ìŠ¤ 5ê°œ ìˆ˜ì§‘
    news_articles = (search_realtime_news(user_query) or [])[:max_keep]

    # 2) ë¹„êµ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì œì™¸)
    # ë¬¸ì„œ í…ìŠ¤íŠ¸
    doc_texts, doc_keep_idx = [], []
    for i, d in enumerate(docs):
        t = _extract_doc_text(d)  # ê¸°ì¡´ ìœ í‹¸
        if t:
            doc_texts.append(t)
            doc_keep_idx.append(i)

    # ë‰´ìŠ¤ í…ìŠ¤íŠ¸(ì œëª©+ë³¸ë¬¸)
    news_texts, news_keep_idx = [], []
    for j, n in enumerate(news_articles):
        title = (n.get("title") or "").strip()
        body  = (n.get("contents") or "").strip()
        combo = (title + "\n" + body).strip()
        if combo:
            news_texts.append(combo)
            news_keep_idx.append(j)

    # ì–´ëŠ í•œìª½ì´ë¼ë„ ë¹„ë©´: ì›ë³¸ ë‰´ìŠ¤(ìƒí•œë§Œ ì ìš©)ì™€ ì˜í–‰ë ¬ + ë¸”ë¡(ì œëª©+ë³¸ë¬¸) ë°˜í™˜
    if not doc_texts or not news_texts:
        fallback = news_articles[:min(max_keep, len(news_articles))]
        lines = []
        for idx, n in enumerate(fallback, 1):
            title = (n.get("title") or "").strip()
            contents = (n.get("contents") or "").strip()
            url = (n.get("url") or "").strip()
            link = f"ğŸ”— [Link]({url})" if url else "ë§í¬ ì—†ìŒ"
            head = f"- [{idx}] {title} Â· {link}"
            if contents:
                lines.append(f"{head}\n  ë³¸ë¬¸:\n  {contents}")
            else:
                lines.append(head)
        news_block = "\n".join(lines)
        return fallback, np.zeros((len(doc_texts), len(news_texts)), dtype=np.float32), news_block

    # 3) ì„ë² ë”© & ìœ ì‚¬ë„ ê³„ì‚° [D x N]
    # ë¬¸ì„œ ì„ë² ë”©: 'vector' ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í•´ë‹¹ ë¬¸ì„œë§Œ ì„ë² ë”©
    doc_vecs_list = []
    to_encode_texts, to_encode_pos = [], []
    for k, gi in enumerate(doc_keep_idx):
        v = docs[gi].get("vector", None)
        if v is None:
            to_encode_pos.append(k)
            to_encode_texts.append(doc_texts[k])
            doc_vecs_list.append(None)
        else:
            doc_vecs_list.append(torch.as_tensor(v, dtype=torch.float32, device=device))

    if to_encode_texts:
        enc = embed_model.encode(to_encode_texts, convert_to_tensor=True, device=device)
        for t_idx, k in enumerate(to_encode_pos):
            doc_vecs_list[k] = enc[t_idx]

    doc_vecs = torch.stack(doc_vecs_list, dim=0)  # [D, dim]
    news_vecs = embed_model.encode(news_texts, convert_to_tensor=True, device=device)
    sim = util.cos_sim(doc_vecs, news_vecs).detach().cpu().numpy()  # (D, N)

    # 4) ë¬¸ì„œë³„ argmax ë‰´ìŠ¤ ì„ íƒ â†’ ì ìˆ˜ ë†’ì€ ë§¤ì¹­ë¶€í„° ì¤‘ë³µ ì œê±° (ë³´ì¶© ì—†ìŒ)
    picks = []  # (doc_global_idx, news_global_idx, score)
    for d_local in range(sim.shape[0]):
        n_local = int(sim[d_local].argmax())
        score   = float(sim[d_local, n_local])
        d_global = doc_keep_idx[d_local]
        n_global = news_keep_idx[n_local]
        picks.append((d_global, n_global, score))

    picks.sort(key=lambda x: -x[2])  # ê°•í•œ ë§¤ì¹­ ìš°ì„ 

    selected, seen = [], set()
    upper = min(max_keep, len(news_articles))
    for _, n_global, _ in picks:
        if n_global not in seen:
            selected.append(n_global)
            seen.add(n_global)
            if len(selected) >= upper:
                break

    filtered_news = [news_articles[j] for j in selected]  # ë³´ì¶© ì—†ìŒ

    # 5) ë‰´ìŠ¤ ë¸”ë¡(ì œëª©, ë³¸ë¬¸ë§Œ, URL)
    lines = []
    for idx, n in enumerate(filtered_news, 1):
        title = (n.get("title") or "").strip()
        contents = (n.get("contents") or "").strip()
        url = (n.get("url") or "").strip()
        link = f"ğŸ”— [Link]({url})" if url else "ë§í¬ ì—†ìŒ"
        lines.append(f"- N{idx}. ì œëª©: {title} Â· {link}\n  ìš”ì•½: {contents}")
    news_block = "\n".join(lines)

    return filtered_news, sim, news_block

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ê²€ìƒ‰ ë¬¸ì„œ ìš”ì•½ (Gemini)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def summarize_documents(
    docs: List[Dict[str, Any]],
    *,
    model_name: str = "gemini-1.5-flash",
    max_output_tokens: int = 200,
    temperature: float = 0.4
) -> Tuple[List[str], str]:
    """ê° ë¬¸ì„œë¥¼ ê°„ê²° ìš”ì•½. (ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ìŠ¤í‚µí•˜ì§€ ì•Šê³  ë¹ˆ ìš”ì•½ìœ¼ë¡œë¼ë„ push ê°€ëŠ¥)"""
    summaries: List[str] = []
    for doc in docs:
        doc_body = _extract_doc_text(doc)
        summary_prompt = f"""
ë‹¤ìŒì€ ì¶•êµ¬ ê·œì •/ì§€ì¹¨ ë¬¸ì„œì˜ í•œ ì„¹ì…˜ì…ë‹ˆë‹¤. ìµœì¢… ë‹µë³€ì—ì„œ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡,
**ì§ì ‘ ëª…ì‹œëœ ì¡°í•­ ë²ˆí˜¸**ë¥¼ ì°¾ì•„ ê°„ê²° ìš”ì•½í•˜ì„¸ìš”.

[ë¬¸ì„œ ë©”íƒ€]
- ë¬¸ì„œëª…: {doc.get('title','').strip()}
- ì¥/ì±•í„°: {doc.get('chapter_title','').strip()}
- ì„¹ì…˜: {doc.get('section_heading','').strip()}

[ë¬¸ì„œ ë³¸ë¬¸]
{doc_body}

[ì‘ì„± ì§€ì¹¨]
1) ë°˜ë“œì‹œ ì•„ë˜ "ìš”ì•½ í˜•ì‹"ì„ ë”°ë¥´ì„¸ìš”. ë‹¤ë¥¸ í˜•ì‹/ë¬¸êµ¬ë¥¼ ì„ì§€ ë§ˆì„¸ìš”.
2) ìš”ì•½ì€ **ë¬¸ì„œì— ì§ì ‘ ë‚˜ì˜¨ ë‚´ìš©ë§Œ** ì‚¬ìš©í•˜ê³ , ì—†ëŠ” ì¡°í•­/ìˆ˜ì¹˜/í•´ì„ì€ ì“°ì§€ ë§ˆì„¸ìš”.
3) **ë¬¸ì„œëª…**ê³¼ **ì¡°í•­ ë²ˆí˜¸**ëŠ” ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”.
4) **ì¡°í•­ ë²ˆí˜¸**ëŠ” ì›ë¬¸ í‘œê¸°(ì˜ˆ: "ì œ14ì¡°", "ì œ14ì¡° ì œ2í•­", "ì œ14ì¡°ì˜2", "ë¶€ì¹™ ì œ1ì¡°")ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
5) ì¡°í•­ ë²ˆí˜¸ê°€ ì—¬ëŸ¬ ê°œë©´ **ì—¬ëŸ¬ ì¤„**ë¡œ ë‚˜ëˆ„ì–´ ê°ê° ìš”ì•½í•˜ì„¸ìš”.
6) ì¡°í•­ ë²ˆí˜¸ê°€ ë³¸ë¬¸ì— **ì•„ì˜ˆ ì—†ìœ¼ë©´** `- (ì¡°í•­ë²ˆí˜¸ ì—†ìŒ): ...`ìœ¼ë¡œ 1ì¤„ ìš”ì•½ë§Œ ì‘ì„±í•˜ì„¸ìš”.
7) ê° ì¤„ì€ **í•œ ë¬¸ì¥**ìœ¼ë¡œ ìš”ì•½í•˜ê³ , í•„ìš”í•˜ë©´ ê´„í˜¸ ì•ˆì— 10~20ì ì´ë‚´ì˜ ì§§ì€ ì›ë¬¸ ì¸ìš©ì„ ë§ë¶™ì—¬ ê²€ì¦ì„±ì„ ë†’ì´ì„¸ìš”(ì„ íƒ).

[ìš”ì•½ í˜•ì‹]
- [ë¬¸ì„œëª…][ì œooì¡°] í•µì‹¬ ìš”ì§€ í•œ ë¬¸ì¥. (ì„ íƒ: "ì§§ì€ ì›ë¬¸ ì¸ìš©")
- [ìƒë²Œ ê·œì •][ì œooì¡° ì œxí•­] í•µì‹¬ ìš”ì§€ í•œ ë¬¸ì¥.
- (ì¡°í•­ë²ˆí˜¸ ì—†ìŒ): ë¬¸ì„œ ì „ì²´ ìš”ì§€ í•œ ë¬¸ì¥.

""".strip()

        out = genai_client.models.generate_content(
            model=model_name,
            contents=summary_prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=max_output_tokens,
                temperature=temperature,
            )
        )
        summaries.append(f"\n{(out.text or '').strip()}")
    return summaries, "\n\n".join(summaries)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ìµœì¢… ë‹µë³€ ìƒì„± (Gemini)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_final_prompt_qa(user_query: str, summary_block: str, system_hint: str, news_block: str = "", history_summary: str = "") -> str:
    """Kë¦¬ê·¸ ê·œì • íŠ¹í™” ì§€ì¹¨ + ìš”ì•½ ë¸”ë¡ìœ¼ë¡œ ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±."""
    return f"""
ê·¼ê±° ì¡°í•­ì„ ëª…ì‹œí•˜ë©° í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.
{system_hint}
ì•„ë˜ ì§€ì¹¨ì„ ì² ì €íˆ ë”°ë¥´ì„¸ìš”:

# [ì´ì „ ëŒ€í™” ë‚´ìš©]
{history_summary or "ì—†ìŒ"}
- ìœ„ 'ì´ì „ ëŒ€í™” ë‚´ìš©'ì€ **í˜„ì¬ ì§ˆë¬¸ê³¼ ì—°ê´€ì„±ì´ ìˆì„ ë•Œë§Œ** ì ê·¹ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.
- ì—°ê´€ì„±ì´ ë‚®ê±°ë‚˜ ì—†ìœ¼ë©´, ì´ì „ ëŒ€í™”ë¥¼ **ì°¸ê³ í•˜ì§€ ë§ê³ ** [ìš”ì•½ëœ ë¬¸ì„œë“¤]/[ë‰´ìŠ¤ë“¤]ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

# [ì—°ê´€ì„± íŒì • ê·œì¹™]
- ë‹¤ìŒ ê²½ìš°ë¥¼ 'ì—°ê´€ì„± ìˆìŒ'ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤:
  1) ë™ì¼/ìœ ì‚¬í•œ ì‚¬ê±´Â·í´ëŸ½Â·ì„ ìˆ˜Â·ê²½ê¸°Â·ëŒ€íšŒÂ·ì¡°í•­ ë²ˆí˜¸Â·ì‹œì¦ŒÂ·ì‹œì ì´ ì¬ë“±ì¥,
  2) ì‚¬ìš©ìì˜ ì§€ì‹œì–´Â·ì§€ì‹œì‚¬(ì˜ˆ: "ê·¸ê±´", "ì•ì—ì„œ ë§í•œ", "ì´ ê²½ìš°")ë¡œ **ì´ì „ í™”ì œ**ë¥¼ ëª…í™•íˆ ì°¸ì¡°,
  3) ì´ì „ ëŒ€í™”ì—ì„œ ì„¤ì •í•œ **ê°€ì •/ì œì•½/ì •ì˜**ë¥¼ í˜„ì¬ ì§ˆë¬¸ì´ ê·¸ëŒ€ë¡œ ì´ì–´ë°›ìŒ.
- ìœ„ ì¡°ê±´ì„ ì¶©ì¡±í•˜ì§€ ì•Šê±°ë‚˜ ëª¨í˜¸í•˜ë©´ 'ì—°ê´€ì„± ë‚®ìŒ/ì—†ìŒ'ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì´ì „ ëŒ€í™”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 'ì—°ê´€ì„± ìˆìŒ'ì´ì–´ë„, **ê·œì •Â·ë¬¸ì„œì™€ ì¶©ëŒ ì‹œ ë¬¸ì„œ/ê·œì •ì„ ìš°ì„ **í•©ë‹ˆë‹¤.

[ì§ˆë¬¸ ë¶„ì„]
1. ì§ˆë¬¸ì´ ìš”êµ¬í•˜ëŠ” í•µì‹¬ ì¶•êµ¬ ê·œì • ìŸì ì„ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”.
2. ì§ˆë¬¸ì´ ë‹¨ìˆœí•œ ê·œì • í™•ì¸ì¸ì§€, ì„ ìˆ˜/í´ëŸ½ ê¶Œë¦¬ ê´€ë ¨ì¸ì§€, ìœ„ë°˜ ì‚¬í•­ íŒë‹¨ì¸ì§€ êµ¬ë¶„í•˜ê³ , ê·¸ì— ë§ëŠ” ê·œì • íŒë‹¨ í¬ì¸íŠ¸ë¥¼ ì‹ë³„í•˜ì„¸ìš”.
3. ì§ˆë¬¸ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ íŠ¹ì • ì¡°í•­ê³¼ ì§ì ‘ ì¼ì¹˜í•˜ì§€ ì•Šë”ë¼ë„, ë¬¸ì„œì— ì–¸ê¸‰ëœ ìœ ì‚¬ ê·œì •, ê·œì • ì²´ê³„, ë˜ëŠ” ê´€ë ¨ ì¡°í•­ê³¼ ë§¥ë½ì ìœ¼ë¡œ ì—°ê²°í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ìˆë‹¤ë©´ ì´ë¥¼ í™œìš©í•´ íŒë‹¨í•˜ì„¸ìš”.
4. ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ êµ¬ì²´ì  ì¡°í•­ì´ ì—†ëŠ” ê²½ìš°, ë¬¸ì„œ ë‚´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ Kë¦¬ê·¸ ê·œì •ì˜ ì¼ë°˜ì ì¸ í•´ì„ ê¸°ì¤€ì— ë”°ë¼ ê°€ëŠ¥í•œ í•´ì„ ë°©í–¥ì„ ë„ì¶œí•˜ì„¸ìš”. ë‹¨, ê²€ìƒ‰ëœ ë¬¸ì„œ ë²”ìœ„ë¥¼ ë„˜ëŠ” ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ì¶”ë¡ ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

[ë¬¸ì„œ ë° ë‰´ìŠ¤ í™œìš© ê¸°ì¤€]
5. ê²€ìƒ‰ëœ ìƒìœ„ 7ê°œ ë¬¸ì„œ ë° ë‰´ìŠ¤ëŠ” ëª¨ë‘ ë‹µë³€ì˜ ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
6. ë¬¸ì„œ ë° ë‰´ìŠ¤ì— **ì§ì ‘ ëª…ì‹œëœ ì¡°í•­ ë²ˆí˜¸**(ì˜ˆ: ì œ1ì¡°, ì œ14ì¡° ë“±)ë§Œ ì¸ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
7. ë‹µë³€ì€ ê°€ê¸‰ì  ì œê³µëœ ë¬¸ì„œ ë° ë‰´ìŠ¤ì—ì„œ ì‹¤ì œë¡œ ëª…ì‹œëœ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, ë¬¸ì„œ ë° ë‰´ìŠ¤ì— ì§ì ‘ì ì¸ í‘œí˜„ì´ ì—†ì„ ê²½ìš°ì—ëŠ” ê·¸ ì·¨ì§€ë‚˜ ìœ ì‚¬í•œ ê·œì •ì˜ ë…¼ë¦¬ë¡œë¶€í„° í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´ì—ì„œ ì¼ë°˜ì ì¸ í•´ì„ì„ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨, í—ˆìœ„ ì¡°í•­ì´ë‚˜ ëª…ë°±íˆ ë¬¸ì„œ ë° ë‰´ìŠ¤ì™€ ë¬´ê´€í•œ ì„ì˜ì˜ í•´ì„ì€ ê¸ˆì§€ë©ë‹ˆë‹¤.
8. ë¬¸ì„œ ë° ë‰´ìŠ¤ì—ì„œ í•´ë‹¹ ë‚´ìš©ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ íŒë‹¨ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì—ëŠ”, ë‹µë³€ì„ íšŒí”¼í•˜ì§€ ë§ê³  ê·¸ ì´ìœ ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.
9. ë‰´ìŠ¤ì— ê¸°ì œëœ ì‹¤ì œ ì‚¬ë¡€ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë¶„ì„ ë‚´ìš©ì„ ëŒ€ì…í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

[ì¶œì²˜ ë° ì‚¬ìš© ì œí•œ]
- ë‹µë³€ì€ [ìš”ì•½ëœ ë¬¸ì„œë“¤]ê³¼ [ë‰´ìŠ¤ë“¤]ì— ì§ì ‘ ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
- ë¬¸ì„œ/ë‰´ìŠ¤ì— ì—†ëŠ” ì¡°í•­Â·ìˆ˜ì¹˜Â·ì‚¬ì‹¤Â·í•´ì„ì„ ìƒˆë¡œ ë§Œë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¶œì²˜ì— ì—†ìœ¼ë©´ â€œì¶œì²˜ì— ì—†ìŒâ€ì´ë¼ê³  ëª…ì‹œí•©ë‹ˆë‹¤.
- ë¬¸ì„œëª…ê³¼ ì¡°í•­ ë²ˆí˜¸ëŠ” ë¬¸ì„œì— ê¸°ì¬ëœ ì›ë¬¸ ë²ˆí˜¸(ì˜ˆ: ìƒë²Œ ê·œì •, ì œ14ì¡°)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

[ì‹ë³„ì ë¶€ì—¬ ê·œì¹™]
- ë¬¸ì„œëŠ” D1~D7ë¡œ ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•˜ë©°, [ìš”ì•½ëœ ë¬¸ì„œë“¤]ì˜ **ìˆœì„œëŒ€ë¡œ** ë§¤ê¹ë‹ˆë‹¤. (ì²« ë¬¸ì„œ = D1)
- ë‰´ìŠ¤ëŠ” N1~N5ë¡œ ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•˜ë©°, [ë‰´ìŠ¤ë“¤]ì˜ **ìˆœì„œëŒ€ë¡œ** ë§¤ê¹ë‹ˆë‹¤. (ì²« ë‰´ìŠ¤ = N1)
- ë³¸ë¬¸ì—ì„œ ì¶œì²˜ê°€ í•„ìš”í•œ ë¬¸ì¥ì€ (D1), (D2), (N1)ì²˜ëŸ¼ ê´„í˜¸ë¡œ ì°¸ì¡°í•©ë‹ˆë‹¤.

[ë‚´ë¶€ ì¶”ë¡  ì ˆì°¨(ì¶œë ¥ ê¸ˆì§€)]
- ì´ ë‹¨ê³„ëŠ” **ë‚´ë¶€ì ìœ¼ë¡œë§Œ ìˆ˜í–‰**í•˜ë©° ìµœì¢… ì¶œë ¥ì—ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
A) ì¦ê±° ìˆ˜ì§‘: D1~D7, N1~N5ì—ì„œ ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì¡°í•­Â·ë¬¸êµ¬Â·ì‚¬ë¡€ë¥¼ ì¶”ì¶œí•˜ê³ , ê° í•­ëª©ì— ì¡°í•­ ë²ˆí˜¸/ë¬¸ì¥ ê·¼ê±°ë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤.
B) ìŸì  ì •ë¦¬: ì‚¬ì‹¤ê´€ê³„(ì‚¬ìš©ì ì§ˆë¬¸ì˜ ìƒí™©)ë¥¼ ìš”ì†Œ(ìš”ê±´/ê¸ˆì§€/ì˜ˆì™¸)ë¡œ ë¶„í•´í•˜ê³  íŒë‹¨í•´ì•¼ í•  í•µì‹¬ ë…¼ì ì„ ë¦¬ìŠ¤íŠ¸ì—…í•©ë‹ˆë‹¤.
C) ê·œì¹™ ì¶”ì¶œÂ·ìš°ì„ ìˆœìœ„: ìƒìœ„ ê·œì •(ì—°ë§¹ ê·œì •/ëŒ€íšŒ ê·œì •/ì§•ê³„ ì½”ë“œ ë“±)â†’ê°œë³„ ì§€ì¹¨â†’ì‚¬ë¡€ ìˆœìœ¼ë¡œ ê¶Œìœ„ë„ë¥¼ ì •í•˜ê³ , ì¶©ëŒ ì‹œ ìƒìœ„ ê·œì •/íŠ¹ë³„ê·œì • ìš°ì„  ì›ì¹™ì„ ì ìš©í•©ë‹ˆë‹¤.
D) ì ìš© í…ŒìŠ¤íŠ¸: ê° ê·œì •ì˜ ìš”ê±´ì„ ì‚¬ì‹¤ê´€ê³„ì— ëŒ€ì…í•˜ì—¬ ì¶©ì¡±/ë¶ˆì¶©ì¡±ì„ í‘œë¡œ ì ê²€í•˜ê³ , ì˜ˆì™¸/ë‹¨ì„œ ì¡°í•­ ìœ ë¬´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
E) ì‚¬ë¡€ ìœ ì¶”(ë‰´ìŠ¤): ë‰´ìŠ¤ì˜ ì‚¬ì‹¤ê´€ê³„ë¥¼ í˜„ì¬ ì‚¬ì•ˆê³¼ **ìœ ì‚¬/ì°¨ì´** ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ(analogize/distinguish)í•˜ì—¬ ê²°ë¡ ì˜ íƒ€ë‹¹ì„±ì„ ë³´ê°•í•©ë‹ˆë‹¤.
F) ê²°ë¡  ì„¤ê³„: ê°€ì¥ ì„¤ë“ë ¥ ìˆëŠ” ê·œì • ê²½ë¡œë¥¼ ì„ íƒí•˜ê³ , ê° ê²°ë¡  ë¬¸ì¥ì— ë¶™ì¼ (Dk)/(Nk) ê·¼ê±°ë¥¼ ë¯¸ë¦¬ ì§€ì •í•©ë‹ˆë‹¤.
G) ê²€ì¦: ë¹„ê²€ì¦ ì£¼ì¥ ì œê±°, ê³¼ë„í•œ ì¼ë°˜í™” ê¸ˆì§€, ëª¨ë“  ì£¼ì¥ì— ìµœì†Œ 1ê°œ ì´ìƒì˜ (Dk)/(Nk) ì—°ê²° ì—¬ë¶€ë¥¼ ì ê²€í•©ë‹ˆë‹¤.

[ì ìš© ë°©ì‹]
10. ë¬¸ì„œ ë‚´ ì¡°í•­ì´ë‚˜ í•´ì„ì´ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ìƒí™©ì— ì–´ë–»ê²Œ ì ìš©ë˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì—°ê²°í•´ ì„¤ëª…í•˜ì„¸ìš”.
11. í•´ë‹¹ ê·œì •ì˜ ëª©ì ì´ë‚˜ ì·¨ì§€ê°€ ì§ˆë¬¸ ì´í•´ì— ë„ì›€ì´ ë  ê²½ìš°, ê°„ë‹¨íˆ ì†Œê°œí•´ë„ ì¢‹ìŠµë‹ˆë‹¤.
12. ì‚¬ì•ˆì˜ ì„±ê²©(ì˜ˆ: ì„ ìˆ˜ ê³„ì•½/ì´ì /ë“±ë¡/ì¬ì •/ì§•ê³„)ì„ êµ¬ë¶„í•˜ì—¬, í˜„ì‹¤ì ì¸ ëŒ€ì‘ ë°©ì•ˆì´ë‚˜ ì ˆì°¨ì  ì„¤ëª…ì„ ê°„ê²°í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”.
13. ë¬¸ì„œ ê°„ ë‚´ìš©ì´ ì¶©ëŒí•˜ê±°ë‚˜ í•´ì„ì— ë‹¤íˆ¼ì˜ ì—¬ì§€ê°€ ìˆì„ ê²½ìš°, "ê·œì • í•´ì„ì— ìŸì ì´ ìˆëŠ” ì‚¬ì•ˆì…ë‹ˆë‹¤."ë¼ê³  ëª…ì‹œí•˜ê³  í•´ì„ ê¸°ì¤€ì´ë‚˜ ì—°ë§¹ì˜ ì¼ë°˜ì ì¸ íŒë‹¨ ê²½í–¥ì„ ê°„ë‹¨íˆ ì œì‹œí•˜ì„¸ìš”.

[í‘œí˜„ ë°©ì‹]
14. ì¶•êµ¬ ê´€ê³„ìë‚˜ ì‹¤ë¬´ìê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡, ëª…ë£Œí•˜ê³  **ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´**ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
15. ìƒë‹´ ë‚´ìš©ì„ **Kë¦¬ê·¸ ê´€ê³„ìë‚˜ ì¶•êµ¬íŒ¬ì—ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒì²˜ëŸ¼** ì‘ì„±í•˜ì„¸ìš”.
16. ì¶•êµ¬ ì „ë¬¸ ìš©ì–´ëŠ” í•„ìš” ì‹œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ë§ë¶™ì´ë˜, ê³¼ë„í•œ ê·œì • ì¡°ë¬¸ì²´ëŠ” í”¼í•˜ì„¸ìš”.
17. ë‹µë³€ì€ ë°˜ë³µì ì¸ ë‚´ìš©ì€ ì‘ì„±í•˜ì§€ ë§ê³  **3~4ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ ** ì‘ì„±í•˜ì„¸ìš”.
18. ê° ë¬¸ë‹¨ì€ **2~3ë¬¸ì¥ ì´ë‚´**ë¡œ **ê°„ê²°í•˜ê²Œ** ì‘ì„±í•˜ì„¸ìš”.
19. **ë¬¸ë‹¨ê³¼ ë¬¸ë‹¨ ì‚¬ì´ëŠ” ë¹ˆ ì¤„ í•œ ì¤„ë¡œ êµ¬ë¶„**í•©ë‹ˆë‹¤.
20. ëª¨ë“  ë¬¸ì¥ì€ **ë§ˆì¹¨í‘œ(.)**ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.

[ì§ˆë¬¸]
{user_query}

[ìš”ì•½ëœ ë¬¸ì„œë“¤]
{summary_block}

[ë‰´ìŠ¤ë“¤]
{news_block}

[ì¶œë ¥ í˜•ì‹ (ë§ˆí¬ë‹¤ìš´)]
1) ## âœ… ìµœì¢… ë‹µë³€ 
   - ë‹µë³€ì˜ ì²« **1~2ë¬¸ì¥**ì„ ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ ìœ¼ë¡œ ë¨¼ì € ì‘ì„±í•©ë‹ˆë‹¤.
   - ì´ì–´ì§€ëŠ” ë‹µë³€ê³¼ ê³µê°„ì´ ë¶„ë¦¬ë˜ê²Œ **êµ¬ë¶„ì„ '--'**ì„ ë„£ìœ¼ì„¸ìš”.
   - ì´ì–´ì§€ëŠ” ë‹µë³€ì€ ìì—°ìŠ¤ëŸ½ê²Œ **3ê°œì˜ ë¬¸ë‹¨**ìœ¼ë¡œ ë‚˜ëˆ  ì‘ì„±í•©ë‹ˆë‹¤.
   - ê° ë¬¸ë‹¨ì€ **2~3ë¬¸ì¥**ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
   - ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” **ë¹ˆ ì¤„ í•œ ì¤„**ì„ ë„£ì–´ êµ¬ë¶„í•©ë‹ˆë‹¤. 
   - í•„ìš”í•œ ë¬¸ì¥ ëì— (Dk)/(Nk)ë¡œ ê·¼ê±°ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.  
   - ëª¨ë“  ë¬¸ì¥ì€ ë§ˆì¹¨í‘œë¡œ ëë‚©ë‹ˆë‹¤.

2) ## ğŸ“š ë‹µë³€ ê·¼ê±° ì¡°í•­
   - **ìµœì¢… ë‹µë³€ ë³¸ë¬¸ì— ì‹¤ì œë¡œ (Dk)ë¡œ ì¸ìš©ëœ ë¬¸ì„œë§Œ** í•œ ì¤„ì”© ì¶œë ¥í•©ë‹ˆë‹¤. (ì¸ìš©ë˜ì§€ ì•Šì€ D ë²ˆí˜¸ëŠ” **ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ**)
   - ê° í•­ëª©ì€ `- Dk. [ì¡°í•­ë²ˆí˜¸] ê°„ë‹¨ ìš”ì§€.` í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
   - ì¡°í•­ë²ˆí˜¸ê°€ ì—¬ëŸ¬ ê°œë©´ `Â·`ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤. `"[ì¶œì²˜ì— ì—†ìŒ]"` ê°™ì€ í”Œë ˆì´ìŠ¤í™€ë”ëŠ” **ê¸ˆì§€**í•©ë‹ˆë‹¤.
   - D1. [ë¬¸ì„œëª…][ì œooì¡°] â€¦ .
   
3) ## ğŸ“° ì°¸ê³  ë‰´ìŠ¤ 
   - **ì´ ì„¹ì…˜ì€ í•­ìƒ ì¶œë ¥í•©ë‹ˆë‹¤.**  
   - ì›ì¹™ì ìœ¼ë¡œ **ìµœì¢… ë‹µë³€ ë³¸ë¬¸ì— (Nk)ë¡œ ì¸ìš©ëœ ë‰´ìŠ¤ë§Œ** ë‚˜ì—´í•©ë‹ˆë‹¤.
   - ë§Œì•½ ë³¸ë¬¸ì— ì–´ë–¤ ë‰´ìŠ¤ë„ ì¸ìš©ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ì‘ì„±í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
   - ê° í•­ëª©ì€  `- Nk. ì œëª© ğŸ”— [Link](URL)`  í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤. (URL ì—†ìœ¼ë©´ `ë§í¬ ì—†ìŒ`)
   - URLì´ [ë‰´ìŠ¤ë“¤]ì— ì—†ìœ¼ë©´ `URL ë¯¸ì œê³µ`ìœ¼ë¡œ í‘œê¸°í•©ë‹ˆë‹¤. ì„ì˜ URL ìƒì„±ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.
   - N1. ë‰´ìŠ¤ ğŸ”— [Link](https://...)
   - ë‰´ìŠ¤ì œëª©ì„ ëª…ì‹œí•˜ë˜ **ì œëª©:** í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ë§ê³  ì˜¤ì§ ì œëª© ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

[ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸]
- ë³¸ë¬¸ì— ìˆëŠ” **ëª¨ë“  (Dk)**ê°€ **ğŸ“š ì„¹ì…˜ì— 1íšŒì”©ë§Œ** ë“±ì¥í•˜ëŠ”ê°€? (ì¤‘ë³µÂ·ëˆ„ë½ ê¸ˆì§€)
- **(Nk)**ë¥¼ ì¼ë‹¤ë©´ ğŸ“° ì„¹ì…˜ì— ë™ì¼í•œ ë²ˆí˜¸ë§Œ ë‚˜ì—´í–ˆëŠ”ê°€? (ì¶”ê°€/ëˆ„ë½ ê¸ˆì§€)
- êµ¬ë¶„ì„ ì€ ë°˜ë“œì‹œ `---` í•œ ì¤„ë¡œ ì¼ëŠ”ê°€?
- **ë²”ìœ„ ì¸ìš©(ì˜ˆ: D1~D7)**ì„ ì“°ì§€ ì•Šì•˜ëŠ”ê°€?

[ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸]
- ê° ì£¼ì¥ì— ëŒ€ì‘ë˜ëŠ” (Dk) ë˜ëŠ” (Nk)ê°€ ìµœì†Œ 1ê°œ ì´ìƒ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
- ë¬¸ì„œ/ë‰´ìŠ¤ì— ì—†ëŠ” ì •ë³´ëŠ” ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.
- í—¤ë”©ê³¼ ë¦¬ìŠ¤íŠ¸ ë“± ë§ˆí¬ë‹¤ìš´ í‘œê¸°ë§Œ ì‚¬ìš©í•˜ê³ , í‘œ/ì½”ë“œë¸”ë¡ì€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- **ë¬¸ì„œ ê·¼ê±° ìš”ì•½ì—ëŠ” ì‹¤ì œ ë³¸ë¬¸ì— ì¸ìš©ëœ Dkë§Œ ì¡´ì¬**í•´ì•¼ í•©ë‹ˆë‹¤. ë¹„ì–´ ìˆëŠ” D4~D7 ê°™ì€ **ë¹ˆ í•­ëª©ì„ ì ˆëŒ€ ë§Œë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤.**
- **ë‰´ìŠ¤ ì°¸ê³ ëŠ” í•­ìƒ ì¶œë ¥**ë˜ì–´ì•¼ í•˜ë©°, ë³¸ë¬¸ì— (Nk)ê°€ ì—†ë‹¤ë©´ **ë³¸ë¬¸ì„ ìˆ˜ì •**í•´ ìµœì†Œ 1ê°œ ì´ìƒì˜ (Nk)ë¥¼ í¬í•¨ì‹œí‚µë‹ˆë‹¤.
- ë‰´ìŠ¤ í•­ëª©ì˜ URLì€ [ë‰´ìŠ¤ë“¤]ì—ì„œ ì œê³µëœ ê°’ë§Œ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ `URL ë¯¸ì œê³µ`ìœ¼ë¡œ í‘œê¸°í•©ë‹ˆë‹¤.
""".strip()

def generate_final_answer_text_qa(
    final_prompt: str,
    *,
    model_name: str = "gemini-1.5-flash",
    max_output_tokens: int = 2000,
    temperature: float = 0.4
) -> str:
    """ìµœì¢… LLM í˜¸ì¶œ."""
    answer = genai_client.models.generate_content(
        model=model_name,
        contents=final_prompt,
        config=types.GenerateContentConfig(
            system_instruction="ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ì¶•êµ¬ ê·œì • ì‹¤ë¬´ìì…ë‹ˆë‹¤.",
            max_output_tokens=max_output_tokens,
            temperature=temperature,
        )
    )
    return (answer.text or "").strip()